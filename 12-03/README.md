# Домашнее задание к занятию "12.3 Развертывание кластера на собственных серверах, лекция 1"

> Поработав с персональным кластером, можно заняться проектами. Вам пришла задача подготовить кластер под новый проект.

## Задание 1: Описать требования к кластеру
> Сначала проекту необходимо определить требуемые ресурсы. Известно, что проекту нужны база данных, система кеширования, а само приложение состоит из бекенда и фронтенда. Опишите, какие ресурсы нужны, если известно:
> 
> * База данных должна быть отказоустойчивой. Потребляет 4 ГБ ОЗУ в работе, 1 ядро. 3 копии.
> * Кэш должен быть отказоустойчивый. Потребляет 4 ГБ ОЗУ в работе, 1 ядро. 3 копии.
> * Фронтенд обрабатывает внешние запросы быстро, отдавая статику. Потребляет не более 50 МБ ОЗУ на каждый экземпляр, 0.2 ядра. 5 копий.
> * Бекенд потребляет 600 МБ ОЗУ и по 1 ядру на копию. 10 копий.

Прежде всего отметим, что размещение СУБД в кластере является плохой практикой. Более правильным будет выделение под СУБД отдельных серверов, без использования виртуализации. Однако для целей данного домашнего задания предположим, что в данном случае СУБД размещается внутри кластера.

### Необходимые ресурсы

* для СУБД потребуется 3 ядра и 12 Гб ОЗУ;
* для кэша -- 3 ядра и 12 Гб ОЗУ;
* для фронтэнда -- достаточно 1 ядро и 500 Мб ОЗУ;
* для бэкэнда -- 10 ядер и 6 Гб ОЗУ.

Всего получаем 17 ядер и 30.5 Гб ОЗУ.

### Служебные ресурсы кластера

Нам будет нужен как минимум один сервер для Control Plane кластера, для которого необходимо 2 ядра и 2 Гб ОЗУ. Поскольку все остальные ресурсы проекта требуют отказоустойчивости, логично заложить отказоустойчивость и в кластер. Это значит, что нам будет нужно как минимум 3 ноды для Control Plane, всего 6 ядер и 6 Гб ОЗУ.

На каждой рабочей ноде также потребуется задействовать 1 ядро и 1 Гб под служебные ресурсы.

### Выбор конфигурации ноды

Нода должна вмещать как минимум один самый большой контейнер. В нашем случае это контейнер СУБД или кэша, 1 ядро и 4 Гб ОЗУ. Однако в этом случае нам будет нужно 17 нод, что с практической точки зрения неудобно (слишком мелкое деление). Более практично сделать ноды большего размера. Мне кажется разумным использовать ноды на 4-8 ядер и 8-16 Гб ОЗУ на каждой.

В минимальной конфигурации 4 ядра и 8 Гб ОЗУ я бы использовал 3 выделенных ноды под Control Plane, а сервисы запускал бы на выделенных рабочих нодах, которых в этом случае будет необходимо 6 штук: 17 ядер на проект плюс 6 ядер (по 1 ядру на каждой рабочей ноде) на служебные задачи. Итого будет нужно 9 нод, причём рабочие ноды будут заняты практически полностью (останется только 1 свободное ядро).

В максимально предложенной конфигурации, 8 ядер и 16 Гб ОЗУ, выделять отдельные ноды под Control Plane кажется расточительным. Для более экономного расходования ресурсов можно использовать эти ноды одновременно в качестве рабочих. В этом случае будет достаточно 4 ноды, при этом останется небольшой запас по ядрам (будет задействовано 17+6+4 = 27 ядер при 32 имеющихся -- 5 ядер в запасе).

В обоих случаях требования по памяти удовлетворяются.

### Обеспечение отказоустойчивости

Для обеспечения отказоустойчивости необходимо иметь в запасе как минимум ещё одну ноду.

Однако для минимальной конфигурации добавление одной ноды добавит в кластер только три свободных ядра. Мне кажется, что для надёжной работы кластера этого мало, и я бы рекомендовал использовать как минимум две резервных ноды в конфигурации 4 ядра и 8 Гб памяти.

В то же время для конфигурации ноды 8 ядер и 16 Гб ОЗУ одной дополнительной ноды будет вполне достаточно (7 свободных ядер при необходимом количестве в 17, хороший процент резервирования), особенно с учётом того, что и без того в кластере уже было 5 свободных ядер.

### Итоговый результат

Итого, получаем следующие два варианта формирования кластера:

* 11 нод в конфигурации 4 ядра и 8 Гб ОЗУ;
* 5 нод в конфигурации 8 ядер и 16 Гб ОЗУ.

В данной конфигурации мне кажется предпочтительным второй вариант, как более простой в конфигурации и менее ресурсоёмкий. Однако в этом варианте Control Plane размещается на тех же нодах, что и сервисы, что не является хорошей практикой.

На практике я бы предпочёл иметь три отдельных ноды для Control Plane на нодах меньшего размера, плюс несколько нод с большим количеством ресурсов в качестве рабочих. В частности, для требуемой в условиях задачи конфигурации это будет 3 ноды по 2 ядра и 4 Гб ОЗУ и 4 ноды по 8 ядер и 16 Гб ОЗУ в качестве рабочих.  
